{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sprint11 深層学習スクラッチ 畳み込みニューラルネットワーク１\n",
    "\n",
    "1次元の畳み込みニューラルネットワークスクラッチ\n",
    "\n",
    "畳み込みニューラルネットワーク（CNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "\n",
    "このSprintでは1次元の 畳み込み層 を作成し、畳み込みの基礎を理解することを目指します。次のSprintでは2次元畳み込み層とプーリング層を作成することで、一般的に画像に対して利用されるCNNを完成させます。\n",
    "\n",
    "\n",
    "クラスの名前はScratch1dCNNClassifierとしてください。クラスの構造などは前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。\n",
    "\n",
    "#### 1次元畳み込み層とは\n",
    "CNNでは画像に対しての2次元畳み込み層が定番ですが、ここでは理解しやすくするためにまずは1次元畳み込み層を実装します。1次元畳み込みは実用上は自然言語や波形データなどの 系列データ で使われることが多いです。\n",
    "\n",
    "\n",
    "畳み込みは任意の次元に対して考えることができ、立体データに対しての3次元畳み込みまではフレームワークで一般的に用意されています。\n",
    "\n",
    "#### データセットの用意\n",
    "検証には引き続きMNISTデータセットを使用します。1次元畳み込みでは全結合のニューラルネットワークと同様に平滑化されたものを入力します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import statistics\n",
    "from scipy import stats\n",
    "import collections\n",
    "import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
    "\n",
    "\n",
    "ここでは パディング は考えず、ストライド も1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。\n",
    "\n",
    "$$\n",
    "a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n",
    "$$\n",
    "\n",
    "$a_i$ : 出力される配列のi番目の値\n",
    "\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "\n",
    "$x_{(i+s)}$ : 入力の配列の(i+s)番目の値\n",
    "\n",
    "\n",
    "$w_s$ : 重みの配列のs番目の値\n",
    "\n",
    "\n",
    "$b$ : バイアス項\n",
    "\n",
    "\n",
    "全てスカラーです。\n",
    "\n",
    "\n",
    "次に更新式です。ここがAdaGradなどに置き換えられる点は全結合層と同様です。\n",
    "\n",
    "$$\n",
    "w_s^{\\prime} = w_s - \\alpha \\frac{\\partial L}{\\partial w_s} \\\\\n",
    "b^{\\prime} = b - \\alpha \\frac{\\partial L}{\\partial b}\n",
    "$$\n",
    "\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_s}$ : $w_s$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b}$ : $b$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "勾配 $\\frac{\\partial L}{\\partial w_s}$ や $\\frac{\\partial L}{\\partial b}$ を求めるためのバックプロパゲーションの数式が以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_s} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}x_{(i+s)}\\\\\n",
    "\\frac{\\partial L}{\\partial b} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial a_i}$ : 勾配の配列のi番目の値\n",
    "\n",
    "\n",
    "$N_{out}$ : 出力のサイズ\n",
    "\n",
    "\n",
    "前の層に流す誤差の数式は以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_j} = \\sum_{s=0}^{F-1} \\frac{\\partial L}{\\partial a_{(j-s)}}w_s\n",
    "$$\n",
    "\n",
    "$$\n",
    "$\\frac{\\partial L}{\\partial x_j}$ : 前の層に流す誤差の配列のj番目の値\n",
    "\n",
    "\n",
    "ただし、 $j-s<0$ または $j-s>N_{out}-1$ のとき $\\frac{\\partial L}{\\partial a_{(j-s)}} =0$ です。\n",
    "\n",
    "\n",
    "全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差を全て足すことで勾配を求めます。計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv1D_1CH:\n",
    "    \"\"\"\n",
    "    チャネル１、スライド１、パディングなし、バッチサイズ１\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init(self):\n",
    "        self.x = None\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        \n",
    "    def forward(self, x, w, b):\n",
    "        # backward用に確保\n",
    "        self.x = x\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        # forward計算\n",
    "        delta = self.x.shape[0] -self.w.shape[0]\n",
    "        a = np.zeros(delta+1)\n",
    "        for n in range(delta+1):\n",
    "            a[n] = (self.x[n:self.w.shape[0]+n]*self.w).sum() + self.b\n",
    "        return a\n",
    "    \n",
    "    def backward(self, delta_a):\n",
    "        # delta_b\n",
    "        delta_b = np.array([delta_a.sum()])\n",
    "        # delta_w\n",
    "        delta_w = np.empty(self.w.shape[0])\n",
    "        for n in range(self.w.shape[0]):\n",
    "            delta_w[n] = delta_a.dot(self.x[n:n+delta_a.shape[0]])\n",
    "        # delta_x\n",
    "        delta_x = np.pad(self.w, [0,1], 'constant')*delta_a[0] + np.pad(self.w, [1,0], 'constant')*delta_a[1] \n",
    "        return delta_b, delta_w, delta_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a :  [35. 50.]\n",
      "delta_b, delta_w, delta_x :  (array([30]), array([ 50.,  80., 110.]), array([ 30, 110, 170, 140]))\n"
     ]
    }
   ],
   "source": [
    "# 問題３についてclassを使用して畳み込み計算\n",
    "\n",
    "# 入力x、重みw、バイアスb\n",
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "# 誤差\n",
    "delta_a = np.array([10, 20])\n",
    "\n",
    "conv1_ch = conv1D_1CH()\n",
    "print(\"a : \", conv1_ch.forward(x, w, b))\n",
    "print(\"delta_b, delta_w, delta_x : \", conv1_ch.backward(delta_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】1次元畳み込み後の出力サイズの計算\n",
    "畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。パディングやストライドも含めています。この計算を行う関数を作成してください。\n",
    "\n",
    "$$\n",
    "N_{out} =  \\frac{N_{in}+2P-F}{S} + 1\\\\\n",
    "$$\n",
    "\n",
    "$N_{out}$ : 出力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$N_{in}$ : 入力のサイズ（特徴量の数）\n",
    "\n",
    "\n",
    "$P$ : ある方向へのパディングの数\n",
    "\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "\n",
    "$S$ : ストライドのサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outsize(n_in, P, F, S):\n",
    "    n_out = (n_in + 2*P - F)/S + 1\n",
    "    if n_out!=int(n_out):\n",
    "        # n_outが割り切れない場合、強制終了\n",
    "        print(\"出力サイズが割り切れません。パディングとストライドを再設定して下さい。\")\n",
    "        return\n",
    "    return int(n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 問題３について出力サイズ計算\n",
    "get_outsize(4, 0, 3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題3】小さな配列での1次元畳み込み層の実験\n",
    "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。\n",
    "\n",
    "\n",
    "入力x、重みw、バイアスbを次のようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力x、重みw、バイアスb\n",
    "x = np.array([1,2,3,4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [35 50]\n"
     ]
    }
   ],
   "source": [
    "# 出力（答え）\n",
    "# a = np.array([35, 50])\n",
    "\n",
    "# 畳み込み計算\n",
    "a = np.array([(x[:3]*w).sum()+1, (x[1:]*w).sum()+1])\n",
    "print(\"a\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 誤差\n",
    "delta_a = np.array([10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_b [30]\n",
      "delta_w [ 50  80 110]\n",
      "delta_x [ 30 110 170 140]\n"
     ]
    }
   ],
   "source": [
    "# バックプロパゲーション（答え）\n",
    "# delta_b = np.array([30])\n",
    "# delta_w = np.array([50, 80, 110])\n",
    "# delta_x = np.array([30, 110, 170, 140])\n",
    "\n",
    "# バックプロパゲーション計算\n",
    "delta_b = np.array([delta_a[0]+delta_a[1]])\n",
    "delta_w = np.array([delta_a[0]*x[0]+delta_a[1]*x[1],\n",
    "                   delta_a[0]*x[1]+delta_a[1]*x[2],\n",
    "                   delta_a[0]*x[2]+delta_a[1]*x[3]])\n",
    "delta_x = np.array([delta_a[0]*w[0],\n",
    "                    delta_a[0]*w[1]+delta_a[1]*w[0],\n",
    "                    delta_a[0]*w[2]+delta_a[1]*w[1],\n",
    "                    delta_a[1]*w[2]])\n",
    "print(\"delta_b\", delta_b)\n",
    "print(\"delta_w\", delta_w)\n",
    "print(\"delta_x\", delta_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。\n",
    "\n",
    "\n",
    "例えば以下のようなx, w, bがあった場合は、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
    "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
    "b = np.array([1, 2, 3]) # （出力チャンネル数）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力は次のようになります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力が2チャンネル、出力が3チャンネルの例です。計算グラフを書いた上で、バックプロパゲーションも手計算で考えてみましょう。計算グラフの中には和と積しか登場しないので、微分を新たに考える必要はありません。\n",
    "\n",
    "《補足》\n",
    "\n",
    "\n",
    "チャンネル数を加える場合、配列をどういう順番にするかという問題があります。(バッチサイズ、チャンネル数、特徴量数)または(バッチサイズ、特徴量数、チャンネル数)が一般的で、ライブラリによって順番は異なっています。（切り替えて使用できるものもあります）\n",
    "\n",
    "\n",
    "今回のスクラッチでは自身の実装上どちらが効率的かを考えて選んでください。上記の例ではバッチサイズは考えておらず、(チャンネル数、特徴量数)です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】（アドバンス課題）パディングの実装\n",
    "畳み込み層にパディングの機能を加えてください。1次元配列の場合、前後にn個特徴量を増やせるようにしてください。\n",
    "\n",
    "\n",
    "最も単純なパディングは全て0で埋める ゼロパディング であり、CNNでは一般的です。他に端の値を繰り返す方法などもあります。\n",
    "\n",
    "\n",
    "フレームワークによっては、元の入力のサイズを保つようにという指定をすることができます。この機能も持たせておくと便利です。なお、NumPyにはパディングの関数が存在します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題6】（アドバンス課題）ミニバッチへの対応\n",
    "ここまでの課題はバッチサイズ1で良いとしてきました。しかし、実際は全結合層同様にミニバッチ学習が行われます。Conv1dクラスを複数のデータが同時に計算できるように変更してください。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題7】（アドバンス課題）任意のストライド数\n",
    "ストライドは1限定の実装をしてきましたが、任意のストライド数に対応できるようにしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題8】学習と推定\n",
    "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。\n",
    "\n",
    "\n",
    "出力層だけは全結合層をそのまま使ってください。ただし、チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、 平滑化 を行なってください。\n",
    "\n",
    "\n",
    "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv1D:\n",
    "    \"\"\"\n",
    "    チャネル複数、ストライド複数、パディング複数、バッチサイズ複数\n",
    "    \n",
    "    下記をコンストラクタに加える\n",
    "    input_shape：（バッチ数、channel数、特徴量数）\n",
    "    w_shape : (出力チャネル、入力チャネル、フィルタサイズ)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, input_shape, w_shape, P, S):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.initializer = initializer\n",
    "        self.W = self.initializer.W(*w_shape)    # (出力チャンネル数、入力チャンネル数、フィルタサイズ)\n",
    "        self.B = self.initializer.B(w_shape[0])    # (出力チャンネル数)\n",
    "        # 初期値\n",
    "        self.Z = None\n",
    "        #  AdaGrad用の変数\n",
    "        self.H_B = 0\n",
    "        self.H_W = 0\n",
    "        # conv初期値\n",
    "        self.input_shape = input_shape\n",
    "        self.w_shape = w_shape\n",
    "        self.P = P\n",
    "        self.S = S\n",
    "        self.n_out = n_nodes2//w_shape[0]\n",
    "    \n",
    "    def forward(self, Z): # Z （バッチサイズ、入力チャンネル数、特徴量）\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        # input_shaepを統一する\n",
    "        if Z.shape!=self.input_shape:\n",
    "            # 予測時はバッチサイズが異なるため１次元目は「−１」\n",
    "            Z = Z.reshape(-1, *self.input_shape[1:])\n",
    "        # backward用に確保\n",
    "        self.Z = Z\n",
    "        # forward計算        \n",
    "        # 出力の初期設定\n",
    "        A = np.zeros([self.Z.shape[0], self.W.shape[0], self.n_out]) # A （バッチサイズ、出力チャンネル数、特徴量数）\n",
    "        # パディングに合わせてxを変換\n",
    "        self.Z = np.pad(self.Z, [(0,0), (0, 0), (self.P, self.P)], 'constant')\n",
    "        # tensordotによるテンソル積(チャネルファーストに合わせたインデックス)\n",
    "        Z_index = [i for i in range(self.Z.ndim)]\n",
    "        W_index = [i for i in range(self.W.ndim)]     \n",
    "        # バッチに対してブロードキャスト\n",
    "        # ストライドに合わせる\n",
    "        for col in range(self.n_out):\n",
    "            A[:, :, col] = np.tensordot(self.Z[:, :, col*self.S:self.W.shape[2]+col*self.S],\n",
    "                                                  self.W, axes=(Z_index[1:], W_index[1:])) + self.B\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        # input_shaepを統一する    \n",
    "        if dA.shape!=(self.Z.shape[0], self.W.shape[0], self.n_out):\n",
    "            dA = dA.reshape(self.Z.shape[0], self.W.shape[0], self.n_out) # (バッチサイズ、出力チャンネル数、特徴量数)\n",
    "        # backwardの計算\n",
    "        # dB\n",
    "        dB = dA.sum(axis=(0, -1))\n",
    "        # dW\n",
    "        # バッチに対してブロードキャスト(dAのチャネル次元を残すように設計)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        for channel in range(dA.shape[1]):\n",
    "            for col in range(dA.shape[-1]):\n",
    "                dW[channel, :, :] += (self.Z[:, :, col*self.S:col*self.S+dW.shape[-1]]\n",
    "                                               *dA[:, channel:channel+1, col:col+1]).sum(axis=0)\n",
    "        # dZ\n",
    "        # バッチに対してブロードキャスト\n",
    "        # ストライドに合わせる\n",
    "        dZ = np.zeros_like(self.Z)\n",
    "        for col in range(dA.shape[-1]):\n",
    "            dZ[:, :, :] += (np.pad(self.W, [(0,0), (0, 0), (col*self.S, (dA.shape[-1]-1-col)*self.S)], 'constant')\n",
    "                               *dA[:, :, col].reshape(dA.shape[0], -1, 1, 1)).sum(axis=1)\n",
    "        # パディングの分を削除\n",
    "        if self.P>0:\n",
    "            dZ = dZ[:, :, self.P:-self.P]\n",
    "        \n",
    "        #  AdaGrad用の計算\n",
    "        size = dA.shape[0] #バッチサイズ\n",
    "        self.H_B += (dB/size)**2\n",
    "        self.H_W += (dW/size)**2\n",
    "        # 更新(AdaGrad用に引数を設定)\n",
    "        self.B, self.W = self.optimizer.update(self.B, self.W, dB, dW, self.H_B, self.H_W, size)\n",
    "\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "    \"\"\"\n",
    "    DNNスクラッチ\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch_count : int\n",
    "      エポック回数\n",
    "    layer_act : dic\n",
    "      layerとact名\n",
    "    n_nodes : タプル int\n",
    "      各FCノードの数\n",
    "    lr : int\n",
    "      学習率\n",
    "    activation : obj\n",
    "      活性化手法のインスタンス\n",
    "    layer_type : obj\n",
    "      層のインスタンス\n",
    "    initializer : obj\n",
    "      初期化方法のインスタンス\n",
    "    optimizer : obj\n",
    "      最適化手法のインスタンス\n",
    "    sigma : float\n",
    "      標準偏差、SimpleInitializerのみ入力\n",
    "    layer_dic : dic\n",
    "      key : layer名、value : 層のインスタンス\n",
    "    act_dic : dic\n",
    "      key : activation名、value : activationのインスタンス\n",
    "    train_loss : list\n",
    "      trainのエポック毎のcross_entropyを格納するリスト(エポック毎の平均値)\n",
    "    val_loss : list\n",
    "      valのエポック毎のcross_entropyを格納するリスト(全データ分を計算)\n",
    "    n_features : int\n",
    "      特徴量の数\n",
    "    n_output : int\n",
    "      出力層のノード数\n",
    "    \"\"\"\n",
    "    def __init__(self, epoch_count, layer_act, n_nodes, lr, layer_type,\n",
    "                      activation,initializer, optimizer, conv_info, sigma=None):\n",
    "        self.epoch_count = epoch_count\n",
    "        self.n_nodes = n_nodes\n",
    "        self.lr = lr\n",
    "        self.sigma = sigma\n",
    "        self.layer_type = layer_type\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.conv_info = conv_info\n",
    "        self.layer_dic = {}\n",
    "        self.act_dic = {}\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        # conv1dのoutsizeを取得(size * channel)\n",
    "        # outsizeが割り切れない場合、強制終了になるよう関数get_outsizeを設定\n",
    "        conv_outsize = get_outsize(self.conv_info[\"in_size\"], self.conv_info[\"P\"],\n",
    "                                                 self.conv_info[\"w_size\"], self.conv_info[\"S\"])*self.conv_info[\"out_ch\"]\n",
    "        # 初期化\n",
    "        ini_sigma = self.sigma\n",
    "        n_features = X.shape[1]\n",
    "        n_output = y.shape[1]\n",
    "        # 特徴量の数、各ノード数、出力ノード数を結合\n",
    "        self.n_nodes = list(self.n_nodes)\n",
    "        self.n_nodes.insert(0, conv_outsize)\n",
    "        self.n_nodes.insert(0, n_features)\n",
    "        self.n_nodes.append(n_output)\n",
    "        # 層の数だけインスタンス化、リストに追加\n",
    "        num = 0\n",
    "        nodes_num = len(self.n_nodes) - 1\n",
    "        for layer, act in layer_act.items():\n",
    "            # インスタンス時にsigma設定しない場合(AdaGrad)、前のノード数を代入する\n",
    "            self.sigma = self.n_nodes[num] if ini_sigma is None else ini_sigma\n",
    "            \n",
    "            # 最初の層をconv1Dでインスタンス化\n",
    "            if num == 0:\n",
    "                # インプット、フィルターのshape設定\n",
    "                input_shape = (self.conv_info[\"in_batch\"], self.conv_info[\"in_ch\"], self.conv_info[\"in_size\"])\n",
    "                w_shape = (self.conv_info[\"out_ch\"], self.conv_info[\"in_ch\"], self.conv_info[\"w_size\"])\n",
    "                # インスタンス化(インプット、フィルター情報もコンストラクタに加える)\n",
    "                self.layer_dic[layer] = conv1D(self.n_nodes[num], self.n_nodes[num+1], self.initializer(self.sigma),\n",
    "                                                             self.optimizer(self.lr), input_shape=input_shape, w_shape=w_shape,\n",
    "                                                             P=self.conv_info[\"P\"], S=self.conv_info[\"S\"])\n",
    "            # その他の層をインスタンス化\n",
    "            else:\n",
    "                self.layer_dic[layer] = self.layer_type(self.n_nodes[num], self.n_nodes[num+1],\n",
    "                                                                       self.initializer(self.sigma), self.optimizer(self.lr))\n",
    "        \n",
    "            # 出力時のみactはSoftmax\n",
    "            if num+1 == nodes_num:\n",
    "                self.act_dic[act] = Softmax()\n",
    "            else:\n",
    "                self.act_dic[act] = self.activation()\n",
    "            num += 1\n",
    "            \n",
    "        # ミニバッチのインスタンス\n",
    "        get_mini_batch = GetMiniBatch(X, y, batch_size=20)\n",
    "        \n",
    "        # エポック回数繰り返し\n",
    "        for epoch in range(self.epoch_count):\n",
    "            # イテレーション回数の初期化\n",
    "            iter_count = 1\n",
    "            # イテレーション毎のloss(cross_entropy)記録用array初期化\n",
    "            train_loss = np.zeros(len(get_mini_batch))\n",
    "            # イテレーション回数繰り返し\n",
    "            for mini_X, mini_y in get_mini_batch:\n",
    "                # 回数の表示\n",
    "                if iter_count%2400 == 0:\n",
    "                    print(\"エポック{}\".format(epoch+1))\n",
    "                    print(\"{}回目の学習\".format(iter_count))\n",
    "                # 層の数だけforward\n",
    "                # 処理回数の初期化\n",
    "                num = 0\n",
    "                for layer, act in zip(self.layer_dic.values(), self.act_dic.values()):\n",
    "                    if num==0:\n",
    "                        Z = mini_X\n",
    "                    \n",
    "                    # 最初の層をconv1D、reshapeしてフラット化したのを出力\n",
    "                    if num==0:\n",
    "                        A = layer.forward(Z).reshape(mini_X.shape[0], -1)\n",
    "                    else:\n",
    "                        A = layer.forward(Z)\n",
    "                    Z = act.forward(A)\n",
    "                    num += 1\n",
    "                   \n",
    "                # 層の数だけbackward(降順)\n",
    "                # 逆順用の辞書を作成\n",
    "                layer_dic_re = {}\n",
    "                act_dic_re = {}\n",
    "                for key, item in sorted(self.layer_dic.items(), reverse=True):\n",
    "                    layer_dic_re[key] = item\n",
    "                for key, item in sorted(self.act_dic.items(), reverse=True):\n",
    "                    act_dic_re[key] = item\n",
    "                # 処理回数の初期化   \n",
    "                num = 0\n",
    "                for layer, act in zip(layer_dic_re.values(), act_dic_re.values()):\n",
    "                    if num==0:\n",
    "                        # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                        dA, train_L = act.backward(mini_y)\n",
    "                    else:\n",
    "                        dA = act.backward(dZ)\n",
    "                    dZ = layer.backward(dA)\n",
    "                    num += 1   \n",
    "                # イテレーション毎のloss(cross_entropy)記録\n",
    "                train_loss[iter_count-1] = train_L\n",
    "                # イテレーション回数のカウント\n",
    "                iter_count += 1\n",
    "             \n",
    "            # valのlossを計算\n",
    "            # valのforward\n",
    "            num = 0\n",
    "            for layer, act in zip(self.layer_dic.values(), self.act_dic.values()):\n",
    "                if num==0:\n",
    "                    Z = X_val\n",
    "\n",
    "                # 最初の層をconv1D、reshapeしてフラット化したのを出力\n",
    "                if num==0:\n",
    "                    A = layer.forward(Z).reshape(X_val.shape[0], -1)\n",
    "                else:\n",
    "                    A = layer.forward(Z)\n",
    "                Z = act.forward(A)\n",
    "               \n",
    "                # valのcross_entropy計算\n",
    "                if num+1 == nodes_num:\n",
    "                    dA, val_L = act.backward(y_val)\n",
    "                num += 1\n",
    "            # loss(cross_entropy)記録\n",
    "            self.train_loss.append(train_loss.mean())\n",
    "            self.val_loss.append(val_L)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        num = 0\n",
    "        for layer, act in zip(self.layer_dic.values(), self.act_dic.values()):\n",
    "            if num==0:\n",
    "                Z = X      \n",
    "            # 最初の層をconv1D、reshapeしてフラット化したのを出力\n",
    "            if num==0:\n",
    "                A = layer.forward(Z).reshape(X.shape[0], -1)\n",
    "            else:\n",
    "                A = layer.forward(Z)\n",
    "            Z = act.forward(A)\n",
    "            num += 1\n",
    "        \n",
    "        return Z.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.initializer = initializer\n",
    "        self.W = self.initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = self.initializer.B(n_nodes2)\n",
    "        # 初期値\n",
    "        self.Z = None\n",
    "        #  AdaGrad用の変数\n",
    "        self.H_B = 0\n",
    "        self.H_W = 0\n",
    "            \n",
    "    def forward(self, Z):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        # backward用に保持\n",
    "        self.Z = Z\n",
    "        # forwardの計算\n",
    "        Z = np.concatenate([np.ones([self.Z.shape[0], 1]), self.Z], axis=1)\n",
    "        W_baias = np.concatenate([self.B.reshape(1, -1), self.W], axis=0)\n",
    "        A = Z.dot(W_baias)\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # backwardの計算\n",
    "        dB = dA.sum(axis=0)\n",
    "        dW = self.Z.T.dot(dA)\n",
    "        dZ = dA.dot(self.W.T)\n",
    "        \n",
    "        #  AdaGrad用の計算\n",
    "        size = dA.shape[0]\n",
    "        self.H_B += (dB/size)**2\n",
    "        self.H_W += (dW/size)**2\n",
    "        \n",
    "        # 更新(AdaGrad用に引数を設定)\n",
    "        self.B, self.W = self.optimizer.update(self.B, self.W, dB, dW, self.H_B, self.H_W, size)\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.A = None\n",
    "    \n",
    "    def forward(self, A):\n",
    "        # backward用に保持\n",
    "        self.A = A\n",
    "        # forward計算\n",
    "        Z = np.tanh(A)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dA = dZ * (1 - (np.tanh(self.A))**2)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "    \n",
    "    def forward(self, A):\n",
    "        # forward計算\n",
    "        Z = np.exp(A)/(np.exp(A).sum(axis=1).reshape([-1, 1]))\n",
    "        # backward用に保持\n",
    "        self.Z = Z\n",
    "        return Z\n",
    "        \n",
    "    def backward(self, y):\n",
    "        # backward\n",
    "        dA = self.Z - y\n",
    "        # cross_entropy\n",
    "        L = -(y*np.log(self.Z)).sum()/y.shape[0]\n",
    "        return dA, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xavier:\n",
    "    \"\"\"\n",
    "    Xavierの初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      Xavierの標準偏差\n",
    "    \n",
    "    FCの場合\n",
    "    n_nodes1 : int\n",
    "      入力ノード数\n",
    "    n_nodes2 : int\n",
    "      出力ノード数\n",
    "      \n",
    "    conv1Dの場合\n",
    "    n_nodes1 : int\n",
    "      出力チャンネル数\n",
    "    n_nodes2 : int\n",
    "      入力チャンネル数\n",
    "    n_size : int\n",
    "      フィルタサイズ\n",
    "      \n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1):\n",
    "        # ScratchDeepNeuralNetrowkClassifierでは変数名sigmaとしてn_nodes1を受け取っている\n",
    "        self.sigma = np.sqrt(1/n_nodes1)\n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2, n_size=None):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        np.random.seed(seed=0)\n",
    "        if n_size:\n",
    "            W = self.sigma * np.random.randn(n_nodes1, n_nodes2, n_size)\n",
    "        else:\n",
    "            W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        np.random.seed(seed=1)\n",
    "        # １次元で出力\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGrad\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, B, W, dB, dW, H_B, H_W, size=None):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        \"\"\"         \n",
    "        B -= self.lr*dB/size/(np.sqrt(H_B) + 1E-1)\n",
    "        W -= self.lr*dW/size/(np.sqrt(H_W) + 1E-1)\n",
    "        \n",
    "        return B, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データロード\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# 平滑化\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# 正規化\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "#分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "# one_hot化\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_val_one_hot = enc.fit_transform(y_val[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "エポック1\n",
      "2400回目の学習\n",
      "エポック2\n",
      "2400回目の学習\n",
      "エポック3\n",
      "2400回目の学習\n",
      "エポック4\n",
      "2400回目の学習\n",
      "エポック5\n",
      "2400回目の学習\n",
      "エポック6\n",
      "2400回目の学習\n",
      "エポック7\n",
      "2400回目の学習\n",
      "エポック8\n",
      "2400回目の学習\n",
      "エポック9\n",
      "2400回目の学習\n",
      "エポック10\n",
      "2400回目の学習\n"
     ]
    }
   ],
   "source": [
    "# 1層目がconv1D、パディングあり、ストライドあり\n",
    "# エポック１０\n",
    "# 入力バッチ数、入力チャネル、入力サイズ、出力チャネル、フィルターサイズ、パディング、スライド\n",
    "conv_info = {\"in_batch\":20, \"in_ch\":1, \"in_size\":784, \"out_ch\":2, \"w_size\":2, \"P\":3, \"S\":2}\n",
    "\n",
    "# FC層のノード数\n",
    "n_nodes = ()\n",
    "\n",
    "# 層構成\n",
    "layer_act = {\"CONV1D\":\"ACT1\", \"FC2\":\"ACT2\"}\n",
    "\n",
    "# Tanh, Xavier, AdaGrad\n",
    "conv1d_PS = ScratchDeepNeuralNetrowkClassifier(epoch_count=10, layer_act=layer_act, n_nodes=n_nodes, lr=0.01,\n",
    "                                               layer_type=FC, activation=Tanh, initializer=Xavier, optimizer=AdaGrad, sigma=None,\n",
    "                                               conv_info=conv_info)\n",
    "conv1d_PS.fit(X_train, y_train_one_hot, X_val, y_val_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解率：0.9155833333333333\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1層目がconv1D　パディングあり、ストライドあり\n",
    "# エポック１０\n",
    "# 推定　→　Accuracy\n",
    "y_pred = conv1d_PS.predict(X_val)\n",
    "print(\"正解率：{}\".format(accuracy_score(y_val, y_pred)))\n",
    "# 学習曲線\n",
    "plt.plot(conv1d_PS.train_loss, label='train')\n",
    "plt.plot(conv1d_PS.val_loss, label='val')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Learning curve / Tanh, Xavier, AdaGrad')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
